# -*- coding: utf-8 -*-
"""perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CH5c43B85APcLXfIk5gLUfVZZKoGyEyJ
"""

# Universidad Nacional Autónoma de México
# Facultad de Ingeniería

# Aguilar Enriquez Paul Sebastian 
# Cabrera Lopez Oscar Emilio 

# Reconocedor de letras

import numpy as np
from math import exp

# Clase para crear la red neuronal

class FullyConnectedLayer:
    """
        Capa de perceptrones
        transfer_function: funcion de transferencia, debe devolver un solo numero
        size: tamaño de la capa (# de neuronas o clases, # de entradas)
        w: una matriz de pesos
    """
    def __init__(self, transfer_function, size=(1,1), w=None):
        self.inputs = size[1] # + 1 # inputs + bias
        self.neurons = size[0]
        if w is None:
            self.w = np.random.random(size=size)
        else:
            self.w = np.asfarray(w)
        self.b = np.ones(self.neurons)
        self.tf = np.vectorize(transfer_function, otypes=[float])

    """
        eval
        inputs: # de entradas
    """
    def eval(self, inputs):
        n = np.dot(self.w, inputs) + self.b
        return self.tf(n)

# Funciones de activación

class TransferFunctions:

    @staticmethod
    def linear(x):
        return x

    @staticmethod
    def satlinear(x):
        if x < 0:
            return 0
        elif x > 1:
            return 1
        else:
            return x

    @staticmethod
    def hardlim(x):
        return 0 if x < 0 else 1
        
    @staticmethod
    def hardlims(x):
        return -1 if x < 0 else 1

    @staticmethod
    def sigmoid(x):
        if x >= 0:
            z = exp(-x)
            return 1 / (1 + z)
        else:
            z = exp(x)
            return z / (1 + z)

    @staticmethod
    def tanh(x):
        if x >= 0:
            z= exp(2*x)
            return (1 - z)/(1 + z)
        else:
            z = exp(2*x)
            (z - 1)/(z + 1)

# Entrenador de la red neuronal

class NeuralNetworkTrainer:
    def __init__(self, net, X, Y, train_ratio=0.7, threshold=0.01, epochs=10):
        self.net = net  # Red
        self.data = {}  # Datos en la red
        self._data_divide(np.asfarray(X), np.asfarray(Y), train_ratio)  # Divide el set de datos en entretenamiento y test
        self.error = 1
        self.data_ratio = train_ratio
        self.threshold = threshold # Umbral de error
        self.epochs = epochs

    def _data_divide(self, X, Y, ratio):
        idx = int(len(X)*ratio)
        self.data["train_X"] = X[0:idx]
        self.data["train_Y"] = Y[0:idx]
        self.data["test_X"]  = X[idx:]
        self.data["test_Y"]  = Y[idx:]
        print("Datos de entrada para entrenamiento:")
        print(self.data["train_X"])
        print("Datos de salida para entrenamiento:")
        print(self.data["train_Y"])


    """
    train: Entrena la red con el set de pruebas
    """
    def train(self):
        epochs = 0
        while self.error > self.threshold and self.epochs > epochs:
            net_error = [] # Error de toda la red durante el epoch

            # Evaluación con la regla del perceptron
            for p, t in zip(self.data["train_X"], self.data["train_Y"]):
                a = self.net.eval(p)
                error = t - a

                # Guardamos el error de la red
                net_error.append(error)

                # Ajuste de pesos y bias
                self.net.w += np.dot(np.reshape(error, (1, error.size)).T, np.reshape(p, (1, p.size)))
                self.net.b += error

            # Error promedio en todo el epoch
            self.error = np.average(np.abs(net_error))
            print(f"Epoch: {epochs} Error: {self.error}")
            epochs += 1

    """
    test: Evalua la red en el set de pruebas

    return(tuple):
        - (np.array): una matrix con los errores de cada neuronas en cada uno de los ejemplos
        - (float): el error promedio
    """
    def test(self):
        print("Datos de entrada para test:")
        print(self.data["test_X"])
        print("Datos de salida para test:")
        print(self.data["test_Y"])
        a, error = [], []
        for p, t in zip(self.data["test_X"], self.data["test_Y"]):
            o = net.eval(p)
            a.append(o)
            error.append(t - o)
        return error, np.average(error)

#entradas de una compuerta
X= [#entrenamiento
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1],
    #pruebas
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]]

#Salidas de una AND, OR y NAND
Y= [
    #entrenamiento
    [0, 0, 1],
    [0, 1, 1],
    [0, 1, 1],
    [1, 1, 0],
    #pruebas
    [0, 0, 1],
    [0, 1, 1],
    [0, 1, 1],
    [1, 1, 0]]

# Entradas para las letras propuestas en la documentacióón ejemplo
X = [
[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0],
[1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1],
[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1],
[1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0],
[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],
[1,1,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1,1,1,0,1,1,0,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1],
[1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0],
[1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0],
[1,1,1,1,1,1,1,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],
[0,0,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,1,1,1,1,0,0],
[1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1],
[1,1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0],
[1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1],
[1,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,1,0,0,1,0,1,1,0,1,1,1,1,0,1,1,0,0,1,1,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1],
[1,1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0],
[1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0],
[1,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,1,1,1,1,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
[0,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,0,1,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0],
[0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,0,1,0,0,0,0,1,1,1,1,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
[0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
[0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
[0,1,0,0,0,0,1,0,0,1,1,0,0,1,1,0,0,1,0,1,1,0,1,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
[0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
[0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
]

# Salidas para las letras propuestas en la documentacióón ejemplo
Y = [
[1,0,0,0,0,0,0,0],
[0,1,0,0,0,0,0,0],
[0,0,1,0,0,0,0,0],
[0,0,0,1,0,0,0,0],
[0,0,0,0,1,0,0,0],
[0,0,0,0,0,1,0,0],
[0,0,0,0,0,0,1,0],
[0,0,0,0,0,0,0,1],
[1,0,0,0,0,0,0,0],
[0,1,0,0,0,0,0,0],
[0,0,1,0,0,0,0,0],
[0,0,0,1,0,0,0,0],
[0,0,0,0,1,0,0,0],
[0,0,0,0,0,1,0,0],
[0,0,0,0,0,0,1,0],
[0,0,0,0,0,0,0,1],
[1,0,0,0,0,0,0,0],
[0,1,0,0,0,0,0,0],
[0,0,1,0,0,0,0,0],
[0,0,0,1,0,0,0,0],
[0,0,0,0,1,0,0,0],
[0,0,0,0,0,1,0,0],
[0,0,0,0,0,0,1,0],
[0,0,0,0,0,0,0,1]
]

# Creamos una red neuronal con funcióón de activación hardlim, con 8 neuronas (son 8 clases) y 64 entradas (el vector es un flat de una matrix de 8x8)
print("Creamos una red neuronal con funcióón de activación hardlim, con 8 neuronas (son 8 clases) y 64 entradas (el vector es un flat de una matrix de 8x8)")
net = FullyConnectedLayer(TransferFunctions.hardlim, size=(8, 64))

# Creamos el entrenador, utilizando dos terceras partes de los datos
print("Creamos el entrenador, utilizando dos terceras partes de los datos, un treshold de 0.1 y 10000 epocas")
trainer = NeuralNetworkTrainer(net, X, Y, train_ratio=(2/3), threshold=0.1, epochs=10000)

# Entrenamos
print("Entrenamos")
print(trainer.train())

# Probamos con el set de prueba
print("Probamos con el set de prueba, los elementos con 1 en el vector es la clase a la que corresponde, y desplegamos el error promedio de la red")
resultados, error = trainer.test()

print("Resultados:")
print(resultados)
print("Error promedio de la red:")
print(error)

print ("Pesos de la red:")
print(net.w)

